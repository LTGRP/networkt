The goal of the imlementation section is to concretely describe how
the software was written, how it all fits together, and what are the
actual operations it performs. The implementation section is a
definition of how the Method was carried out on a programmatic level.

\section{The Database Structure}
The database structure was designed to mimic the nature of the twitter
network graph as closely as possible. For this reason, the datastore
of choice for this project was Neo4j. Neo4j was chosen because of its
powerful ability to model graphs and networks. Neo4j ships with native
implementations to connect nodes together, as well as cypher, a
powerful query language. As an example, cypher is used to select the
tweets of all the individuals in a transnational network, using just
one command.

The database contains several types of objects to represent different
parts of the twitter network. The objects will explained below.

\section{Graph}
The Graph package contains all of the important definitions for the
database structure, access, and extraction of data from Twitter. It is
composed of three important modules:

\begin{enumerate}
\item Data Model
\item Filter Node
\item Network Scrape
\end{enumerate}

\subsection{Data Model}
The data model is where the representations for all of the objects on
Twitter are stored. The representations are designed to capture all of
the useful information available from the Twitter API in an indexable
and computable way.

\subsubsection{The Node Class}
The Node Class represents a node within the system. A node, within
this context, refers to a user on Twitter. The Node contains all of
the information that is available within the Twitter public API. The
following class description shows which fields are available.

\begin{lstlisting}
class Node(StructuredNode):
    name = StringProperty()
    screen_name = StringProperty(unique_index=True)
    
    created_at = StringProperty()
    description = StringProperty()
    favorites_count = IntegerProperty()
    followers_count = IntegerProperty()
    friends_count = IntegerProperty()
    id_str = StringProperty()
    lang = StringProperty()
    listed_count = IntegerProperty()
    location = StringProperty()
    statuses_count = IntegerProperty()
    time_zone = StringProperty()
    utc_offset = IntegerProperty()
    verified = BooleanProperty()
    
    # Relationship definitions
    friends = RelationshipTo('Node', 'FRIEND')
    followers = RelationshipTo('Node', 'FOLLOWER')
    statuses = RelationshipTo('Status', 'STATUS')
    tags = RelationshipTo('Tag', 'TAG')
\end{lstlisting}

This object is used to model a given Twitter user and can be used to
persist all of the important information about them. By breaking apart
the user model into fields of interest, we are able to write queries
that target specific properties of users- for example- we can write a
query to return all users with a location of X.

\subsubsection{The  Status Class}
The Status class is responsible for maintaining all of the statuses of
every user within the network. That is, the status class is a
representation of a Tweet. This class is important because it contains
all of the data that is fed into our clustering sequence for
identifying the diffusion of information. The Status class has a one
to many relationship with the Node class- a node object has many
status objects.

\begin{lstlisting}
class Status(StructuredNode):
  id_str = StringProperty(unique_index=True)

  coordinate_longitude = StringProperty()
  coordinate_latitude = StringProperty()
  created_at = StringProperty()
  date = DateTimeProperty()
  favorite_count = IntegerProperty()
  in_reply_to_screen_name = StringProperty()
  in_reply_to_status_id_str = StringProperty()
  in_reply_to_user_id_str = StringProperty()
  lang = StringProperty()
  possibly_sensitive = BooleanProperty()
  quoted_status_id_str = StringProperty()
  retweet_count = IntegerProperty()
  retweeted = BooleanProperty()
  source = StringProperty()
  text = StringProperty()
  truncated = BooleanProperty()
\end{lstlisting}

As with the Node class, the Status class represents and classifies all
information available about a given Tweet published on Twitter.

\subsection{Filter Node}
The filter node module is responsible for all operations relating to
classifying different nodes during the network scraping
process. Within filter node there are a set of functions called
``filter x'' that accept a Node object as input, and return a true or
false value as to whether a given Node qualifies for a particular
attribute.

To provide a concrete example of Node filtering, consider
identification of Transnational Entrepreneurs. Due to the Twitter
API's rate limits, different filtering strategies are used to
determine the liklihood that a given user is a Transnational
Entrpreneur. This helps improve the performance of collecting networks
of interest on Twitter. During the initial collection of followers of
a startup hub, the filter\_1 function, will determine whether a sample
network of a user qualifies them as being a Transnational candidate.

\subsection{Network Scrape}
The network scrape module contains a set of functions for extracting a
network of a Twitter user in a reliable way. Within network scrape,
there are functions available to:

\begin{enumerate}
\item Get a single User and persist them
\item Get a User's ego-centric follower network and persist them
\item Get a User's ego-centric friend network and persist them
\item Get a User's Status' and persist them
\end{enumerate}

Together, composting these functions allows for a snapshot of a given
individual's network. By abstracting away the functions of the Twitter
API in the Network Scrape module, extracting a user's Twitter network
is significantly eased.

There are several constraints that the Network Scrape module
considers. One of the primary constraints is, that there are different
rate limits for different operations within the Twitter API, therefore
to respect the rate limits and avoid being rate limited, it is
necessary for the program to adjust the amount of time it waits
between each operation. Another constraint which provides significant
problems is that the Twitter API may sometimes confirm the existence
of a user by returning them as a friend or follower of a given user,
but simultaneously prohibit the collection of data via the API for
that user. If this behavior is not accounted for, the Network
representation within Neo4j may contain empty Nodes, or Nodes without
statuses. These problems and others are all abstracted by Network
Scrape, so that the Scrapet package runner module can easily extract
and persist a network snapshot from Twitter to Neo4j.

\section{Cluster}
The cluster package is responsible for determining whether two Tweets
are semantically related, and how semantically related they are. This
information is then used to attempt to trace the diffusion of
information through a given Transnational Entrepreneur's network. In
the code samples below, the implementation, and rationale for design
decisions will be explained.

\begin{lstlisting}
def process_text(text, stem=False):
    """Tokenize text and stem words removing punctuation
    """
    transtable = {ord(s): None for s in string.punctuation}
    transtable[ord('/')] = u''
    text = text.translate(transtable)
    tokens = word_tokenize(text)
    
    if stem:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(t) for t in tokens]
    
    return tokens
\end{lstlisting}

The process text function is responsible for taking a piece of text
and normalizing it for comparison. It starts by first removing all
punctuation, while true, that punctuation carries meaning, it
introduces noise into the clustering algorithm and makes it more
difficult to cluster relevant documents.

After processing the text to remove the punctuation, the words are
stemmed. The process of stemming ensures that two related words are
reduced into their base form. This transforms words like ``macaw'' and
``parakeet'' both to ``bird'' which allows for an easier comparision
by the clustering algorithm.

Finally, the process text function returns an array of ``tokens'',
another way of saying ``words'' which comprise the document that is
processed. These ``tokens'' are then fed into a function which
turns them into a vector, and then clusters them. That function
is below:
  
\begin{lstlisting}
def cluster_documents(documents):
    """Transform texts to Tf-Idf coordinates and cluster texts using
    DBSCAN
    
    """
    vectorizer = TfidfVectorizer(tokenizer=process_text,
                                 stop_words=stopwords.words('english'),
                                 min_df=0.0,
                                 max_df=0.95,
                                 lowercase=True)
    
    # Data Vectorizing
    tfidf_model = vectorizer.fit_transform(documents)
    
    # Data Clustering
    db = DBSCAN(eps=1.50, min_samples=3).fit(tfidf_model)
    
    return db.labels_
\end{lstlisting}



\begin{lstlisting}
def main():
    tag = Tag.nodes.get(name=Tag.FILTER_1)
    for node in tag.users:
        print('processing {}'.format(node.screen_name))
        diffusion_instances = 0
        
        # get tweets of transnational user
        query = (
            ' MATCH (user:Node {{screen_name:"{}"}})'.format(node.screen_name) +
            ' MATCH (user)-[:STATUS]->(statuses)'
            ' RETURN statuses.text, statuses.date, user.screen_name')
        
        statuses, meta = db.cypher_query(query)
        
        for index, status in enumerate(statuses):
            print('{}, {}/{}'.format(node.screen_name, index, len(statuses)), end='\r')
            
            # five days in unix time
            time_delta = 60 * 60 * 24 * 5
            
            # collect friend statuses before status
            query = (
                ' MATCH (user:Node {{screen_name:"{name}"}})'
                ' MATCH (user)-[:FRIEND]->(connections)'
                ' MATCH (connections)-[:STATUS]->(statuses)'
                ' MATCH (statuses {{lang:"en"}})'
                ' MATCH (statuses)<-[:STATUS]-(nodes)'
                ' WHERE statuses.date > {min_date} and statuses.date < {max_date}'
                ' RETURN statuses.text, statuses.date, nodes.screen_name'
                ' ORDER BY statuses.date DESC').format(
                    name=node.screen_name,
                    min_date=status[1] - time_delta,
                    max_date=status[1])
            friend_statuses, meta = db.cypher_query(query)
            
            # collect follow statuses posted after status
            query = (
                ' MATCH (user:Node {{screen_name:"{name}"}})'
                ' MATCH (user)-[:FOLLOWER]->(connections)'
                ' MATCH (connections)-[:STATUS]->(statuses)'
                ' MATCH (statuses {{lang:"en"}})'
                ' MATCH (statuses)<-[:STATUS]-(nodes)'
                ' WHERE statuses.date > {min_date} and statuses.date < {max_date}'
                ' RETURN statuses.text, statuses.date, nodes.screen_name'
                ' ORDER BY statuses.date DESC').format(
                    name=node.screen_name,
                    min_date=status[1],
                    max_date=status[1] + time_delta)
            follower_statuses, meta = db.cypher_query(query)
            
            if(friend_statuses and follower_statuses):
                # cluster and identify diffusion
                if identify_transnational_diffusion(node, len(friend_statuses),
                                                    friend_statuses + [status] + follower_statuses):
                    diffusion_instances += 1
        
        print('{} diffusion instances: {}'.format(node.screen_name, diffusion_instances))


def identify_transnational_diffusion(user, user_index, all_statuses, output=False):
    # extract document from every status and cluster
    clusters = cluster_documents([status[0] for status in all_statuses])
    
    # add cluster id to every document
    results = [l + [r] for l, r in zip(all_statuses, clusters)]
    
    # if user status is not related to any other statuses, return
    if results[user_index][3] == -1:
        return
    
    # gather statuses with same clustering before
    before_statuses = [result for result in results[:user_index]
                       if result[3] == results[user_index][3]]
    
    # gather statuses with same clustering after
    after_statuses = [result for result in results[user_index + 1:]
                      if result[3] == results[user_index][3]]
    
    if before_statuses and after_statuses and output:
        print('=' * 40)
        print('-' * 20, 'before')
        print(before_statuses)
        print('-' * 20, 'status')
        print(results[user_index])
        print('-' * 20, 'after')
        print(after_statuses)
    
    if before_statuses and after_statuses:
        return True
\end{lstlisting}
