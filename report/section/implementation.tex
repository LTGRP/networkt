The implementation section concretely describes how the software was
written, how it all fits together, and what are the actual operations
it performs. The implementation section is a definition of how the
Method was carried out on a computational level.

\section{Database Structure}
The database structure was designed to mimic the nature of the Twitter
network graph as closely as possible. For this reason, the database of
choice for this project was Neo4j. Neo4j was chosen because of its
powerful ability to model graphs and networks. Neo4j ships with native
implementations to connect nodes together, as well as cypher, a
powerful query language. As an example, cypher is used to select the
Tweets of all the individuals in a transnational network, using just
one query.

\subsection{Data Model}
The data model is where the representations for all of the objects on
Twitter are stored. The representations are designed to capture all of
the useful information available from the Twitter API in an indexable
and computable way.

\subsubsection{The Node Class}
The Node Class represents a node within the system. A node, within
this context, refers to a user on Twitter. The Node contains all of
the information that is available within the Twitter public API. The
following class description shows which fields are available.

\begin{lstlisting}
class Node(StructuredNode):
    name = StringProperty()
    screen_name = StringProperty(unique_index=True)
    
    created_at = StringProperty()
    description = StringProperty()
    favorites_count = IntegerProperty()
    followers_count = IntegerProperty()
    friends_count = IntegerProperty()
    id_str = StringProperty()
    lang = StringProperty()
    listed_count = IntegerProperty()
    location = StringProperty()
    statuses_count = IntegerProperty()
    time_zone = StringProperty()
    utc_offset = IntegerProperty()
    verified = BooleanProperty()
    
    # Relationship definitions
    friends = RelationshipTo('Node', 'FRIEND')
    followers = RelationshipTo('Node', 'FOLLOWER')
    statuses = RelationshipTo('Status', 'STATUS')
    tags = RelationshipTo('Tag', 'TAG')
\end{lstlisting}

This object is used to model a given Twitter user and can be used to
persist all of the important information about them. By breaking apart
the user model into fields of interest, we are able to write queries
that target specific properties of users- for example- we can write a
query to return all users with a location of X, X being a location of
our choice.

\subsubsection{The  Status Class}
The Status class is responsible for maintaining all of the statuses of
every user within the network. That is, the Status Class is a
representation of a Tweet. This class is important because it contains
all of the data that is fed into our clustering sequence for
identifying the diffusion of information. The Status class has a one
to many relationship with the Node class- a node object has many
status objects.

\begin{lstlisting}
class Status(StructuredNode):
  id_str = StringProperty(unique_index=True)

  coordinate_longitude = StringProperty()
  coordinate_latitude = StringProperty()
  created_at = StringProperty()
  date = DateTimeProperty()
  favorite_count = IntegerProperty()
  in_reply_to_screen_name = StringProperty()
  in_reply_to_status_id_str = StringProperty()
  in_reply_to_user_id_str = StringProperty()
  lang = StringProperty()
  possibly_sensitive = BooleanProperty()
  quoted_status_id_str = StringProperty()
  retweet_count = IntegerProperty()
  retweeted = BooleanProperty()
  source = StringProperty()
  text = StringProperty()
  truncated = BooleanProperty()
\end{lstlisting}

As with the Node class, the Status class represents and classifies all
information available about a given Tweet published on Twitter.

\section{Extraction}
Following the definition of the Data model, the next package within
the programs pipeline is population the database from the Twitter API.
To do this, the Network Scrape module contains a set of functions for
extracting a network of a Twitter user in a reliable way. Within
network scrape, there are functions available to:

\begin{enumerate}
\item Get a single User and persist them
\item Get a User's egocentric follower network and persist them
\item Get a User's egocentric friend network and persist them
\item Get a User's Status' and persist them
\end{enumerate}

Together, composing these functions allows for a snapshot of a given
individual's network. By abstracting away the functions of the Twitter
API in the Network Scrape module, extracting a user's Twitter network
is significantly simplified.

There are several constraints that the Network Scrape module
considers. One of the primary constraints is, that there are different
rate limits for different operations within the Twitter API, therefore
to respect the rate limits and avoid being rate limited, it is
necessary for the program to adjust the amount of time it waits
between each operation. Another constraint which provides significant
problems is that the Twitter API may sometimes confirm the existence
of a user by returning them as a friend or follower of a given user,
but simultaneously prohibit the collection of data via the API for
that user. If this behavior is not accounted for, the Network
representation within Neo4j may contain empty Nodes, or Nodes without
statuses. These problems and others are all abstracted by Network
Scrape, so that the main runner module can easily extract and persist
a network snapshot from Twitter to Neo4j.

\section{Filtering}
Throughout the extraction process described above, there are a set of
Filters that can be used to moderate and guide the extraction process.
The filter node module is responsible for all of these moderating
operations.  Within filter node there are a set of functions called
``filter x'' (where X is an arbitrary string) that accept a Node data
object (a Twitter user in this context) as input, and return a true or
false value as to whether a given Node qualifies for a particular
attribute.

To provide a concrete example of Node filtering, consider
identification of Transnational Entrepreneurs. Due to the Twitter
API's rate limits, different filtering strategies are used to
determine the likelihood that a given user is a Transnational
Entrepreneur. This helps improve the performance of collecting networks
of interest on Twitter. During the initial collection of followers of
a startup hub, the filter\_1 function, will determine whether a sample
network from a given user qualifies them as being a Transnational
candidate.

\section{Clustering}
The final step in the analysis pipeline involves clustering all of the
resolved data from the network to attempt to identify Transnational
Diffusion. The cluster package is responsible for determining whether
two Tweets are semantically related, and how semantically related they
are. This information is then used to attempt to trace the diffusion
of information through a given Transnational Entrepreneur's
network. In the code samples below, the implementation, and rationale
for design decisions will be explained.

\begin{lstlisting}
def process_text(text, stem=False):
    """Tokenize text and stem words removing punctuation
    """
    transtable = {ord(s): None for s in string.punctuation}
    transtable[ord('/')] = u''
    text = text.translate(transtable)
    tokens = word_tokenize(text)
    
    if stem:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(t) for t in tokens]
    
    return tokens
\end{lstlisting}

The first step of the clustering process involves basic text
processing.  The process text function is responsible for taking a
piece of text and normalizing it for comparison. It starts by first
removing all punctuation.

After processing the text to remove the punctuation, the words are
stemmed. The process of stemming ensures that two related words are
reduced into their base form. This transforms words like ``macaw'' and
``parakeet'' both to ``bird'' which allows for an easier comparison
by the clustering algorithm.

Finally, the process text function returns an array of ``tokens'',
another way of saying ``words'' which comprise the document that is
processed. These ``tokens'' are then fed into a function which turns
them into a vector, and then clusters them. That function is below:
  
\begin{lstlisting}
def cluster_documents(documents):
    """Transform texts to Tf-Idf coordinates and cluster texts using
    DBSCAN
    
    """
    vectorizer = TfidfVectorizer(tokenizer=process_text,
                                 stop_words=stopwords.words('english'),
                                 min_df=0.0,
                                 max_df=0.95,
                                 lowercase=True)
    
    # Data Vectorizing
    tfidf_model = vectorizer.fit_transform(documents)
    
    # Data Clustering
    db = DBSCAN(eps=1.50, min_samples=3).fit(tfidf_model)
    
    return db.labels_
\end{lstlisting}

The cluster documents function operates on a set of documents, and
returns the corresponding clusters of documents. It does this by first
transforming every single document into a vector. The vector is created
by doing term frequency, inverse document frequency analysis (TF-IDF).

TF-IDF analysis is: ``combin[ing] the definitions of term frequency
and inverse document frequency, to produce a composite weight for each
term in each document.''.\cite{informationretrieval} Said another way,
TF-IDF is a way of weighing the importance of a given term within a
document. A term is more important when it appears more frequently
within a document, and when it appears in fewer documents of a
collection.

After the TF-IDF weighing, the terms are clustered using the DB Scan
clustering algorithm. DB Scan works by plotting all vectors on a 2D
space. After all of the vectors have been plotted, each individual
vector is examined for nearby vectors, if a nearby vector is within
some distance (eps, epsilon), then it forms a contiguous group. If
enough vectors within a contiguous group is greater than a threshold
``min\_samples'', then that contiguous group forms a cluster.

Following the ability to process text and cluster it, there needs to
be a faculty for identifying diffusion. To identify diffusion, there
is a function named ``identify\_transnational\_diffusion''. The
function is provided below:

\begin{lstlisting}
def identify_transnational_diffusion(user, user_index, all_statuses, output=False):
    # extract document from every status and cluster
    clusters = cluster_documents([status[0] for status in all_statuses])
    
    # add cluster id to every document
    results = [l + [r] for l, r in zip(all_statuses, clusters)]
    
    # if user status is not related to any other statuses, return
    if results[user_index][3] == -1:
        return
    
    # gather statuses with same clustering before
    before_statuses = [result for result in results[:user_index]
                       if result[3] == results[user_index][3]]
    
    # gather statuses with same clustering after
    after_statuses = [result for result in results[user_index + 1:]
                      if result[3] == results[user_index][3]]
    
    if before_statuses and after_statuses:
        return before_statuses, results[user_index], after_statuses
\end{lstlisting}

The function first clusters all of the Tweets from the friends of a
Transnational before a diffusion event (Tweet). It then clusters all
of the Tweets from the followers of a Transnational Entrepreneur after
the diffusion event. Finally, it groups all Tweets from both groups
(friends and followers), that are of the same content cluster as the
Transnational Entrepreneur's diffusion Tweet.

Together, all of these functions work together to identify
Transnational Diffusion. A high level overview of the operations is
delineated below:

\begin{enumerate}
\item Find Transnational Tweets within the Database, Iterate over each
  Tweet as X
\item Find Transnational Friend Tweets occurring before Tweet X as A
\item Find Transnational Follower Tweets occurring after Tweet X as B
\item Process and cluster Text from Tweets in group A, B, and Tweet X
  together
\item Identify all Tweets from group A, B that are of the same content
  cluster as X
\item Save results of Transnational Diffusion by recording Tweets from
  group A, X, and group B if and only if they share the same content
  cluster as X
\end{enumerate}

When executed in this order, the complete pipeline can find and
extract instances of Transnational Diffusion.
