The goal of the imlementation section is to concretely describe how
the software was written, how it all fits together, and what are the
actual operations it performs. The implementation section is a
definition of how the Method was carried out on a programmatic level.

\section{The Database Structure}
The database structure was designed to mimic the nature of the twitter
network graph as closely as possible. For this reason, the ORM of
choice for this project was Neo4j. Neo4j was chosen because of its
powerful ability to model graphs and networks. It comes with native
implementations to connect nodes together, as well as cypher, a
powerful query language used to query the database. As an example,
cypher is used to select the tweets of all the individuals in a
transnational network, using just one command.

The database contains several types of objects to represent different
parts of the twitter network. The objects will explained below.

\subsection{The Declarative Base: The Node Class}
The Node Class represents a node within the system. A node, within
this context, refers to a user on Twitter. The Node contains all of
the information that is available within the Twitter public API. The
following class description shows which fields are available.

\begin{lstlisting}
  class Node(Base):
    __tablename__ = 'node'
    id = Column(Integer, primary_key=True)
    created_at = Column(Text)
    description = Column(Text)
    favorites_count = Column(Integer)
    followers_count = Column(Integer)
    friends_count = Column(Integer)
    id_str = Column(Text)
    lang = Column(Text)
    listed_count = Column(Integer)
    location = Column(Text)
    name = Column(Text)
    screen_name = Column(Text)
    statuses_count = Column(Integer)
    time_zone = Column(Text)
    utc_offset = Column(Integer)
    verified = Column(Boolean)

    # Filtering Levels
    filter_0 = Column(Boolean)
    filter_1 = Column(Boolean)
    filter_2 = Column(Boolean)
    # Relationship to Status Updates
    statuses = relationship("Status", order_by="Status.date",
                            backref="node", cascade="all, delete")
\end{lstlisting}

This object is used to model a given Twitter user and can be used to
persist all of the important information about them. By breaking apart
the user model into fields of interest, we are able to write queries
that target specific properties of users- for example- we can write a
query to return all users with a location of X.

\subsection{The Declarative Base: The  Status Class}
The Status class is responsible for maintaining all of the statuses of
every user within the network. That is, the status class is a
representation of a Tweet. This class is important because it contains
all of the data that is fed into our clustering sequence for
identifying the diffusion of information. The Status class has a one
to many relationship with the Node class, that is- a node object has
many status objects.

\begin{lstlisting}
class Status(Base):
    __tablename__ = 'status'
    id = Column(Integer, primary_key=True)
    # Parent
    node_id = Column(Integer, ForeignKey('node.id'))
    # Fields
    coordinate_longitude = Column(Text)
    coordinate_latitude = Column(Text)
    created_at = Column(Text)
    date = Column(DateTime)
    favorite_count = Column(Integer)
    id_str = Column(Text)
    in_reply_to_screen_name = Column(Text)
    in_reply_to_status_id_str = Column(Text)
    in_reply_to_user_id_str = Column(Text)
    lang = Column(Text)
    possibly_sensitive = Column(Boolean)
    quoted_status_id_str = Column(Text)
    retweet_count = Column(Integer)
    retweeted = Column(Boolean)
    source = Column(Text)
    text = Column(Text)
    truncated = Column(Boolean)
\end{lstlisting}

As with the Node class, the Status class represents and classifies all
information available about a given Tweet published on Twitter.

\section{Graph}
The Graph package contains all of the important definitions for the
database structure, access, and extraction of data from Twitter. It is
composed of three important files: data\_model.py, filter\_node.py,
network\_scrape.py.


\section{Scrapet}
Scrapet is the tool that is responsible for pulling the data from the
Twitter API and making the appropriate graphs. Scrapet is the core
behind all of the tools in the project. Every single project will end
up using a Scrapet dump of data for rendering, machine learning, or
any other processes necessary for analysis. This tool is composed of a
number of components which will be briefly be introduced
below. Following the introduction and description of components, the
high level architecture will be explained.

\subsection{Scrapet.ini}
The Ini file is important for saving many configuration details.

\subsection{Logger}
The logger is the most important component of the Scrapet system. The
logger is an abstract entity that is either fulfilled as a console
logger, or as a GUI logger depending on the flavor and execution
method of the Scrapet build. The logger is responsible for reporting
on the overall progress and the activity of the system.

\subsection{Runner}
The runner is the main entry point of the system. Whether running from
the GUI mode, or from the command line mode, Scrapet always begins
here. This is where all of the scraping algorithms and functions are
organized.

\subsection{Main}
Main is aptly named as the main entry point into the program. This is
where the GUI version of Scrapet begins. Just like the command line
program though, the true entry point of execution is in Runner. During
execution of the scraping process, scrapet launches 'Runner' as a
thread.

\subsection{Settings Panel}
The settings panel provides an abstract way to edit the ini file which
controls the behavior of the runner. In this way it is possible to use
the GUI to set the parameters that the command line program will
operate with, as they launch the same subprocess - directly or
otherwise.

\section{Cluster}
The best way to understand the cluster package is to go through it
line by line.

\begin{lstlisting}
import os
...
\end{lstlisting}
