The breakdown of the implementation section of this paper will break
apart every package into a section. Subsequently every single module
will be a subsection. At the top of the section will be a brief
package definition (what the package does, how it does it) on a high
level.

\section{The Database Structure}
The database structure was designed to mimic the nature of the twitter
network graph as closely as possible.

\subsection{OGM \& Design Decisions}
The ORM of choice for this project was Neo4j.

\subsection{The Declarative Base: The Node Class}
The Node Class represents a node within the system. It contains all of
the information that is available within the Twitter public API. The
following class description shows which fields are available.

\begin{lstlisting}
  class Node(Base):
    __tablename__ = 'node'
    id = Column(Integer, primary_key=True)
    created_at = Column(Text)
    description = Column(Text)
    favorites_count = Column(Integer)
    followers_count = Column(Integer)
    friends_count = Column(Integer)
    id_str = Column(Text)
    lang = Column(Text)
    listed_count = Column(Integer)
    location = Column(Text)
    name = Column(Text)
    screen_name = Column(Text)
    statuses_count = Column(Integer)
    time_zone = Column(Text)
    utc_offset = Column(Integer)
    verified = Column(Boolean)

    # Filtering Levels
    filter_0 = Column(Boolean)
    filter_1 = Column(Boolean)
    filter_2 = Column(Boolean)
    # Relationship to Status Updates
    statuses = relationship("Status", order_by="Status.date",
                            backref="node", cascade="all, delete")
\end{lstlisting}

\subsection{The Declarative Base: The  Status Class}
The Status class is responsible for maintaining all of the statuses of
every user within the network. This class is important because it
contains all of the data that is fed into our clustering sequence for
identifying the diffusion of information. The Status class has a one
to many relationship with the Node class, that is- a node object has
many status objects.

\begin{lstlisting}
class Status(Base):
    __tablename__ = 'status'
    id = Column(Integer, primary_key=True)
    # Parent
    node_id = Column(Integer, ForeignKey('node.id'))
    # Fields
    coordinate_longitude = Column(Text)
    coordinate_latitude = Column(Text)
    created_at = Column(Text)
    date = Column(DateTime)
    favorite_count = Column(Integer)
    id_str = Column(Text)
    in_reply_to_screen_name = Column(Text)
    in_reply_to_status_id_str = Column(Text)
    in_reply_to_user_id_str = Column(Text)
    lang = Column(Text)
    possibly_sensitive = Column(Boolean)
    quoted_status_id_str = Column(Text)
    retweet_count = Column(Integer)
    retweeted = Column(Boolean)
    source = Column(Text)
    text = Column(Text)
    truncated = Column(Boolean)
\end{lstlisting}

\section{Graph}
The Graph package contains all of the important definitions for the
database structure, access, and extraction of data from Twitter. It is
composed of three important files: data\_model.py, filter\_node.py,
network\_scrape.py.


\section{Scrapet}
Scrapet is the tool that is responsible for pulling the data from the
Twitter API and making the appropriate graphs. Scrapet is the core
behind all of the tools in the project. Every single project will end
up using a Scrapet dump of data for rendering, machine learning, or
any other processes necessary for analysis. This tool is composed of a
number of components which will be briefly be introduced
below. Following the introduction and description of components, the
high level architecture will be explained.

\subsection{Scrapet.ini}
The Ini file is important for saving many configuration details.

\subsection{Logger}
The logger is the most important component of the Scrapet system. The
logger is an abstract entity that is either fulfilled as a console
logger, or as a GUI logger depending on the flavor and execution
method of the Scrapet build. The logger is responsible for reporting
on the overall progress and the activity of the system.

\subsection{Runner}
The runner is the main entry point of the system. Whether running from
the GUI mode, or from the command line mode, Scrapet always begins
here. This is where all of the scraping algorithms and functions are
organized.

\subsection{Main}
Main is aptly named as the main entry point into the program. This is
where the GUI version of Scrapet begins. Just like the command line
program though, the true entry point of execution is in Runner. During
execution of the scraping process, scrapet launches 'Runner' as a
thread.

\subsection{Settings Panel}
The settings panel provides an abstract way to edit the ini file which
controls the behavior of the runner. In this way it is possible to use
the GUI to set the parameters that the command line program will
operate with, as they launch the same subprocess - directly or
otherwise.

\section{Cluster}
The best way to understand the cluster package is to go through it
line by line.

\begin{lstlisting}
import os
...
\end{lstlisting}
