The goal of the imlementation section is to concretely describe how
the software was written, how it all fits together, and what are the
actual operations it performs. The implementation section is a
definition of how the Method was carried out on a programmatic level.

\section{The Database Structure}
The database structure was designed to mimic the nature of the twitter
network graph as closely as possible. For this reason, the datastore
of choice for this project was Neo4j. Neo4j was chosen because of its
powerful ability to model graphs and networks. Neo4j ships with native
implementations to connect nodes together, as well as cypher, a
powerful query language. As an example, cypher is used to select the
tweets of all the individuals in a transnational network, using just
one command.

The database contains several types of objects to represent different
parts of the twitter network. The objects will explained below.

\section{Graph}
The Graph package contains all of the important definitions for the
database structure, access, and extraction of data from Twitter. It is
composed of three important files:

\begin{enumerate}
\item data model
\item filter node
\item network scrape
\end{enumerate}

\subsection{Data Model}
The data model is where the representations for all of the objects on
Twitter are stored. The representations are designed to capture all of
the useful information available from the Twitter API in an indexable
and computable way.

\subsubsection{The Node Class}
The Node Class represents a node within the system. A node, within
this context, refers to a user on Twitter. The Node contains all of
the information that is available within the Twitter public API. The
following class description shows which fields are available.

\begin{lstlisting}
class Node(StructuredNode):
    name = StringProperty()
    screen_name = StringProperty(unique_index=True)
    
    created_at = StringProperty()
    description = StringProperty()
    favorites_count = IntegerProperty()
    followers_count = IntegerProperty()
    friends_count = IntegerProperty()
    id_str = StringProperty()
    lang = StringProperty()
    listed_count = IntegerProperty()
    location = StringProperty()
    statuses_count = IntegerProperty()
    time_zone = StringProperty()
    utc_offset = IntegerProperty()
    verified = BooleanProperty()
    
    # Relationship definitions
    friends = RelationshipTo('Node', 'FRIEND')
    followers = RelationshipTo('Node', 'FOLLOWER')
    statuses = RelationshipTo('Status', 'STATUS')
    tags = RelationshipTo('Tag', 'TAG')
\end{lstlisting}

This object is used to model a given Twitter user and can be used to
persist all of the important information about them. By breaking apart
the user model into fields of interest, we are able to write queries
that target specific properties of users- for example- we can write a
query to return all users with a location of X.

\subsubsection{The  Status Class}
The Status class is responsible for maintaining all of the statuses of
every user within the network. That is, the status class is a
representation of a Tweet. This class is important because it contains
all of the data that is fed into our clustering sequence for
identifying the diffusion of information. The Status class has a one
to many relationship with the Node class- a node object has
many status objects.

\begin{lstlisting}
class Status(StructuredNode):
  id_str = StringProperty(unique_index=True)

  coordinate_longitude = StringProperty()
  coordinate_latitude = StringProperty()
  created_at = StringProperty()
  date = DateTimeProperty()
  favorite_count = IntegerProperty()
  in_reply_to_screen_name = StringProperty()
  in_reply_to_status_id_str = StringProperty()
  in_reply_to_user_id_str = StringProperty()
  lang = StringProperty()
  possibly_sensitive = BooleanProperty()
  quoted_status_id_str = StringProperty()
  retweet_count = IntegerProperty()
  retweeted = BooleanProperty()
  source = StringProperty()
  text = StringProperty()
  truncated = BooleanProperty()
\end{lstlisting}

As with the Node class, the Status class represents and classifies all
information available about a given Tweet published on Twitter.

\subsection{Filter Node}
The filter node module is responsible for all operations relating to
classifying different nodes during the network scraping process. Within
filter node there are a set of functions called ``filter x'' that accept
a Node object as input, and return a true or false value as to whether a
given Node qualifies for a particular attribute.

To provide a concrete example of Node filtering, consider identification
of Transnational Entrepreneurs. Due to the Twitter API's rate limits,
different filtering strategies are used to determine the liklihood that
a given user is a Transnational Entrpreneur. This helps improve the performance
of collecting networks of interest on Twitter. During the initial collection
of followers of a startup hub, the filter\_1 function, will determine whether
a sample network of a user qualifies them as being a Transnational candidate.

\subsection{Network Scrape}

\section{Scrapet}
Scrapet is the tool that is responsible for pulling the data from the
Twitter API and making the appropriate graphs. Scrapet is the core
behind all of the tools in the project. Every single project will end
up using a Scrapet dump of data for rendering, machine learning, or
any other processes necessary for analysis. This tool is composed of a
number of components which will be briefly be introduced
below. Following the introduction and description of components, the
high level architecture will be explained.

\subsection{Scrapet.ini}
The Ini file is important for saving many configuration details.

\subsection{Logger}
The logger is the most important component of the Scrapet system. The
logger is an abstract entity that is either fulfilled as a console
logger, or as a GUI logger depending on the flavor and execution
method of the Scrapet build. The logger is responsible for reporting
on the overall progress and the activity of the system.

\subsection{Runner}
The runner is the main entry point of the system. Whether running from
the GUI mode, or from the command line mode, Scrapet always begins
here. This is where all of the scraping algorithms and functions are
organized.

\subsection{Main}
Main is aptly named as the main entry point into the program. This is
where the GUI version of Scrapet begins. Just like the command line
program though, the true entry point of execution is in Runner. During
execution of the scraping process, scrapet launches 'Runner' as a
thread.

\subsection{Settings Panel}
The settings panel provides an abstract way to edit the ini file which
controls the behavior of the runner. In this way it is possible to use
the GUI to set the parameters that the command line program will
operate with, as they launch the same subprocess - directly or
otherwise.

\section{Cluster}
The best way to understand the cluster package is to go through it
line by line.

\begin{lstlisting}
import os
...
\end{lstlisting}
